from langchain_ibm import ChatWatsonx
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.tools.retriever import create_retriever_tool
import os 
from dotenv import load_dotenv
from typing_extensions import TypedDict

from ibm_watsonx_gov.evaluators.agentic_evaluator import AgenticEvaluator
from ibm_watsonx_gov.config import AgenticAIConfiguration
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langgraph.graph import START, END, StateGraph
import uuid
import json 

from langchain_ibm import ChatWatsonx, WatsonxEmbeddings

from langchain_chroma import Chroma
from colorama import Fore 
import uuid

# RAG Triad: https://myscale.com/blog/ultimate-guide-to-evaluate-rag-system/

load_dotenv()

embeddings = WatsonxEmbeddings(
    model_id="sentence-transformers/all-minilm-l6-v2",
    project_id=os.getenv("WATSONX_PROJECT_ID"),
)

vector_store = Chroma(
    collection_name="credit_card",
    persist_directory="./credit_card",
    embedding_function=embeddings,
)

llm = ChatWatsonx(
    model_id="ibm/granite-3-2b-instruct",
    url=os.getenv("WATSONX_URL", "https://us-south.ml.cloud.ibm.com"),
    apikey=os.getenv("WATSONX_APIKEY"),
    project_id=os.getenv("WATSONX_PROJECT_ID"),
    params={
        "decoding_method": "greedy",
        "temperature": 0,
        "min_new_tokens": 5,
        "max_new_tokens": 250,
        "stop_sequences": ["Human:", "Observation:"],
})

class GraphState(TypedDict):
    """
    Represents the state of our RAG graph.
    
    Attributes:
        input_text (str): The user's raw input query or question
        record_id (str): Unique identifier for the record
        local_context (list[str]): Context retrieved from vector store
        generated_text (str): The final output generated by the LLM
    """
    input_text: str
    record_id: str 
    local_context: list[str]
    generated_text: str
    ground_truth: str 

# Setup evaluator
evaluator = AgenticEvaluator()

# Configuration for context relevance evaluation
context_relevance_config = {
    "question_field": "input_text", 
    "context_fields": ["local_context"], 
    "record_id_field": "record_id", 
}

@evaluator.evaluate_context_relevance(configuration=AgenticAIConfiguration(**context_relevance_config))
def retrieval_node(state: GraphState, config: RunnableConfig):
    similarity_threshold_retriever = vector_store.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={"k": 3, "score_threshold": 0.1}
    )
    context = similarity_threshold_retriever.invoke(state["input_text"])
    print(Fore.LIGHTBLUE_EX + f"üìÑ Retrieved {len(context)} documents from vector store" + Fore.RESET)
    return {
        "local_context": [doc.page_content for doc in context]
    }

# Configuration for faithfulness and answer relevance evaluation
faithfulness_config = {
    "question_field": "input_text", 
    "context_fields": ["local_context"], 
    "record_id_field": "record_id", 
    "output_fields": ["generated_text"],
}

answer_relevance_config = {
    "question_field": "input_text", 
    "context_fields": ["local_context"], 
    "record_id_field": "record_id", 
    "output_fields": ["generated_text"],
    "reference_fields": ["ground_truth"]
}

@evaluator.evaluate_faithfulness(configuration=AgenticAIConfiguration(**faithfulness_config))
@evaluator.evaluate_answer_relevance(configuration=AgenticAIConfiguration(**answer_relevance_config))
def generate_node(state: GraphState, config: RunnableConfig):
    print(Fore.LIGHTYELLOW_EX + f"üß† Generating response for: {state['input_text']}" + Fore.RESET)
    generate_prompt = ChatPromptTemplate.from_template(
        "Answer the following question based on the given context:\n"
        "Context: {context}\n"
        "Question: {input_text}\n"
        "Answer:"
    )

    formatted_prompt = generate_prompt.invoke(
        {"input_text": state["input_text"], "context": "\n".join(state["local_context"])})

    result = llm.invoke(formatted_prompt)
    print(Fore.LIGHTGREEN_EX + f"‚úÖ Generated response: {result.content[:100]}..." + Fore.RESET)
    return {
        "generated_text": result.content
    }

# Build graph
graph_builder = StateGraph(GraphState)
graph_builder.add_node("retrieval", retrieval_node)
graph_builder.add_node("generation", generate_node)

# Add edges
graph_builder.add_edge(START, "retrieval")
graph_builder.add_edge("retrieval", "generation")
graph_builder.add_edge("generation", END)

# Compile the graph
rag_app = graph_builder.compile()

if __name__ == '__main__':
    config = {"configurable": {"thread_id": "abc123"}}
    
    print(Fore.LIGHTMAGENTA_EX + "üöÄ RAG Triad Evaluation System Started!" + Fore.RESET)
    print(Fore.CYAN + "This system evaluates: Context Relevance, Faithfulness, and Answer Relevance" + Fore.RESET)
    print(Fore.CYAN + "Type 'quit', 'exit', or 'q' to stop\n" + Fore.RESET)
    
    while True:
        prompt = input(Fore.LIGHTCYAN_EX + "‚ö°Ô∏è Enter your query: " + Fore.RESET)
        if prompt.lower() in ["quit", "exit", "q"]:
            print(Fore.LIGHTMAGENTA_EX + "üëã Goodbye!" + Fore.RESET)
            break
        
        evaluator.start_run()
        
        # Create unique IDs for this interaction
        record_id = str(uuid.uuid1())
        
        # Invoke the graph
        result = rag_app.invoke({
            "input_text": prompt,
            "record_id": record_id,
            "local_context": [],
            "generated_text": "",
            "ground_truth":""
        }, config=config)
        
        # Display final result
        print(Fore.LIGHTMAGENTA_EX + f"ü§ñ Final Answer: {result['generated_text']}" + Fore.RESET)
        print(Fore.LIGHTCYAN_EX + f"üìö Context Used: {len(result['local_context'])} documents\n" + Fore.RESET)
        
        evaluator.end_run()
        
        # Save evaluation results
        eval_result = evaluator.get_result() 
        with open(f'traces/{uuid.uuid1()}.json', 'w') as f: 
            json.dump(eval_result.get_aggregated_metrics_results(), f)